{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research Report for GA DSI2 Project 4:\n",
    "## By: Yen Wei Loh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executive Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The aim of this report was to predict salaries for data-science related jobs, using data scraped off the website Seek.com.au. BeautifulSoup was used to scrape the data off 55 pages of data-science related job postings in Sydney, resulting in 1080 job postings. Correlation matrices were used to identify keywords and features that indicate a high salary (defined by a salary more than or equal to $80,000 a year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data science is a high growth industry, as modern industry starts to realise the need for data science techniques to deal with the massive amounts of data being generated by an increasingly connected world.\n",
    "\n",
    "### As an aspiring data scientist, it would be an interesting exercise to apply the data gathering and analysing techniques learned during the past 8 weeks at General Assembly to predict the salaries of data-science related jobs in Sydney.\n",
    "\n",
    "### For ease of processing, the scope of this report is limited to the Sydney area, and salaries were classified into 0 (no data), 1 (< $80,000 per annum) and 2 (>$80,000 per annum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful soup was used to scrape data from Seek.com.au, and a total of 1,080 job postings were recorded. After cleaning, the title and descriptions of each job posting were broken down into keywords. These keywords were used as features for use in predicting salary.\n",
    "\n",
    "### Logistic regression, K-nearest neighbours and decision tree classifiers were used to predict salaries for the job postings without salary data, using salary data and features of the job postings that did include salary information.\n",
    "\n",
    "### Logistic regression uses the keywords obtained from title and description to estimate probability of salary being 0, 1 or 2.\n",
    "\n",
    "### K-nearest neighbours uses information from datapoints most similar to it, and uses those to guess the salary for that specific job_posting.\n",
    "\n",
    "### Decision tree classifiers are essentially a series of yes/no questions that are applied to each job posting (e.g does it contain the word \"scientist\"?) Each individual decision goes down a different pathway, eventually leading to the algorithm deciding to apply a salary classification of 0, 1 or 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline accuracy for the dataset was calculated to be 73.7% This means that if we had a model that guessed 0 for salary for each case, it would be correct 73.7% of the time.\n",
    "\n",
    "### Logistic regression produced an accuracy of 77.8%, slightly more accurate than baseline. \n",
    "### Unfortunately, I was unable to tune the KNN model, resulting in a model that only guessed 0 for every case, essentially being equal to baseline.\n",
    "\n",
    "### Decision tree classifiers up to a depth of 3 were used, meaning each decision path had three branching nodes. This however, resulted in a maximum accuracy of 73.3%, even worse than baseline.\n",
    "\n",
    "### Correlation matrices calculated for each keyword against salary data did not show strong correlations for any single feature, with consultant services showing the strongest correlation at 6.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As there were no individual strong correlation, my next step would be to test a PCA model on the data. PCA stands for Principal Component Analysis, and works by converting a set of possibly inter-correlated variables into a set of linearly uncorrelated variables called principal components. These principal components are structured so taht the first principal components explains the largest variance of the data. In theory, all variance in the data can be accounted for if there are a sufficient number of principal components.\n",
    "\n",
    "### In this way, it often reduces the dimensionality of the data, and could produce an even more accurate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
